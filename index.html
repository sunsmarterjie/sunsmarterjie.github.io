<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Yunjie Tian â€” Computer Vision & Multimodal Learning" />
  <title>Yunjie Tian Â· ç”°è¿æ°</title>

  <!-- Open Graph / Twitter -->
  <meta property="og:title" content="Yunjie Tian â€” Computer Vision & Multimodal Learning" />
  <meta property="og:description" content="Postdoctoral Associate at University at Buffalo. Research in representation learning, object detection, and multimodal LLMs." />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="files/susu.jpg" />
  <meta name="twitter:card" content="summary_large_image" />

  <!-- Favicon (optional) -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ğŸ§ </text></svg>">

  <!-- ENHANCED CSS - æ›´é«˜çº§ã€æ›´è€³ç›®ä¸€æ–° -->
  <style>
    :root {
      --bg: #0b0d10;
      --bg-elev: #12161b;
      --text: #e6e8eb;
      --muted: #9aa3ad;
      --link: #7cc4ff;
      --accent: #7c5cff;
      --chip: #212733;
      --card: #10151b;
      --glow: 0 0 20px rgba(124, 92, 255, 0.3); /* æ–°å¢ï¼šå…‰æ™•æ•ˆæœ */
      color-scheme: light dark;
    }

    @media (prefers-color-scheme: light) {
      :root {
        --bg: #ffffff;
        --bg-elev: #f6f8fa;
        --text: #0b1220;
        --muted: #5b6673;
        --link: #0b6bcb;
        --accent: #5b46ff;
        --chip: #eef2f7;
        --card: #ffffff;
        --glow: 0 0 20px rgba(91, 70, 255, 0.1);
      }
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    html {
      scroll-behavior: smooth; /* æ–°å¢ï¼šå¹³æ»‘æ»šåŠ¨ */
    }

    body {
      background: var(--bg);
      color: var(--text);
      font: 16px/1.7 system-ui, -apple-system, Segoe UI, Roboto, Inter, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
      overflow-x: hidden;
    }

    a {
      color: var(--link);
      text-decoration: none;
      transition: all 0.2s ease; /* æ–°å¢ï¼šé“¾æ¥è¿‡æ¸¡æ•ˆæœ */
    }

    a:hover {
      text-decoration: underline;
      color: var(--accent); /* æ–°å¢ï¼šæ‚¬åœæ—¶å˜ä¸ºä¸»è‰²è°ƒ */
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      border-radius: 14px;
      transition: transform 0.3s ease; /* æ–°å¢ï¼šå›¾ç‰‡ç¼©æ”¾è¿‡æ¸¡ */
    }

    img:hover {
      transform: scale(1.02); /* æ–°å¢ï¼šè½»å¾®æ”¾å¤§ */
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 24px;
    }

    header {
      position: sticky;
      top: 0;
      background: color-mix(in oklab, var(--bg) 92%, transparent);
      backdrop-filter: saturate(1.2) blur(10px);
      border-bottom: 1px solid color-mix(in oklab, var(--text) 10%, transparent);
      z-index: 50;
      transition: background 0.3s ease; /* æ–°å¢ï¼šèƒŒæ™¯è¿‡æ¸¡ */
    }

    .nav {
      display: flex;
      align-items: center;
      gap: 16px;
      justify-content: space-between;
      padding: 12px 0;
    }

    .nav a {
      padding: 10px 12px;
      border-radius: 10px;
      position: relative;
      transition: all 0.2s ease;
    }

    .nav a:hover {
      background: color-mix(in oklab, var(--text) 10%, transparent);
      transform: translateY(-1px); /* æ–°å¢ï¼šè½»å¾®ä¸Šæµ® */
    }

    .brand {
      display: flex;
      gap: 12px;
      align-items: center;
    }

    .brand .dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: var(--accent);
      box-shadow: 0 0 24px var(--accent);
      transition: all 0.3s ease; /* æ–°å¢ï¼šå…‰ç‚¹åŠ¨ç”» */
    }

    .brand .dot:hover {
      transform: scale(1.2);
      box-shadow: 0 0 32px var(--accent);
    }

    .grid {
      display: grid;
      gap: 28px;
    }

    @media (min-width: 820px) {
      .grid.cols-2 {
        grid-template-columns: 1.25fr 0.9fr;
      }
    }

    /* HERO SECTION - æ ¸å¿ƒå‡çº§åŒº */
    .hero {
      padding: 40px 0;
      position: relative;
      overflow: hidden;
    }

    .hero::before {
      /* æ–°å¢ï¼šå¾®å¦™çš„è£…é¥°æ€§çº¿æ¡ */
      content: "";
      position: absolute;
      top: 20%;
      left: -5%;
      width: 120%;
      height: 1px;
      background: linear-gradient(90deg, transparent, var(--accent) 50%, transparent);
      opacity: 0.1;
      animation: shimmer 3s infinite;
    }

    @keyframes shimmer {
      0% { transform: translateX(-100%); }
      100% { transform: translateX(100%); }
    }

    .title {
      font-size: clamp(32px, 5vw, 48px);
      font-weight: 800;
      line-height: 1.1;
      margin: 0 0 12px;
      letter-spacing: -0.5px;
      background: linear-gradient(to right, var(--text), var(--link));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .subtitle {
      margin: 6px 0 20px;
      color: var(--muted);
      font-size: 1.1em;
    }

    .chips {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin: 16px 0;
    }

    .chip {
      font-size: 13px;
      padding: 8px 14px;
      border-radius: 999px;
      background: var(--chip);
      color: var(--text);
      border: 1px solid color-mix(in oklab, var(--text) 10%, transparent);
      transition: all 0.2s ease;
      position: relative;
      overflow: hidden;
    }

    .chip::before {
      /* æ–°å¢ï¼šæ‚¬åœæ—¶çš„èƒŒæ™¯å¡«å……åŠ¨ç”» */
      content: "";
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: color-mix(in oklab, var(--accent) 20%, transparent);
      transition: left 0.3s ease;
      z-index: -1;
    }

    .chip:hover::before {
      left: 0;
    }

    .chip:hover {
      border-color: var(--accent);
      transform: translateY(-1px);
      box-shadow: var(--glow);
    }

    .actions {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-top: 20px;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 18px;
      border-radius: 14px;
      border: 1px solid color-mix(in oklab, var(--text) 14%, transparent);
      background: var(--card);
      transition: all 0.2s ease;
      font-weight: 500;
    }

    .btn:hover {
      transform: translateY(-2px);
      border-color: var(--accent);
      box-shadow: var(--glow);
      background: color-mix(in oklab, var(--accent) 10%, var(--card));
    }

    .btn img {
      transition: none; /* é¿å…å›¾æ ‡ä¹Ÿç¼©æ”¾ */
    }

    section {
      margin: 40px 0;
      scroll-margin-top: 80px; /* ä¸ºé”šç‚¹é“¾æ¥ç•™å‡ºç©ºé—´ */
    }

    h2 {
      font-size: clamp(24px, 4vw, 32px);
      font-weight: 700;
      margin: 0 0 20px;
      position: relative;
      padding-bottom: 8px;
    }

    h2::after {
      /* æ–°å¢ï¼šæ ‡é¢˜ä¸‹åˆ’çº¿ */
      content: "";
      position: absolute;
      bottom: 0;
      left: 0;
      width: 60px;
      height: 3px;
      background: var(--accent);
      border-radius: 3px;
    }

    .card {
      background: var(--card);
      border: 1px solid color-mix(in oklab, var(--text) 10%, transparent);
      border-radius: 18px;
      padding: 24px;
      transition: all 0.3s ease;
    }

    .card:hover {
      transform: translateY(-4px);
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
      border-color: color-mix(in oklab, var(--accent) 50%, transparent);
    }

    @media (prefers-color-scheme: light) {
      .card:hover {
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.05);
      }
    }

    /* PUBLICATIONS */
    .pubs {
      display: grid;
      grid-template-columns: 1fr;
      gap: 20px;
    }

    @media (min-width: 740px) {
      .pubs {
        grid-template-columns: 1fr 1fr;
      }
    }

    .pub {
      display: grid;
      grid-template-columns: 120px 1fr;
      gap: 16px;
      align-items: start;
      border: 1px solid color-mix(in oklab, var(--text) 10%, transparent);
      border-radius: 16px;
      padding: 16px;
      background: var(--bg-elev);
      transition: all 0.3s ease;
    }

    .pub:hover {
      transform: translateY(-2px);
      border-color: var(--accent);
      box-shadow: var(--glow);
    }

    .pub .thumb {
      width: 120px;
      aspect-ratio: 4/3;
      object-fit: cover;
      border-radius: 12px;
      background: linear-gradient(45deg, #1a1a1a, #333);
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }

    .pub .meta b {
      font-weight: 700;
    }

    .meta .authors {
      font-size: 14px;
      color: var(--muted);
      line-height: 1.5;
    }

    /* æ–°å¢ï¼šé«˜äº®ä½ çš„åå­— */
    .meta .authors b {
      color: var(--text);
      background: color-mix(in oklab, var(--accent) 30%, transparent);
      padding: 2px 4px;
      border-radius: 4px;
    }

    .meta .venue {
      display: inline-flex;
      gap: 8px;
      align-items: center;
      margin: 8px 0;
    }

    .venue .pill {
      font-size: 12px;
      padding: 6px 12px;
      border-radius: 999px;
      background: var(--chip);
      border: 1px solid color-mix(in oklab, var(--text) 10%, transparent);
      font-weight: 600;
      color: var(--accent);
    }

    .links {
      display: flex;
      gap: 12px;
      flex-wrap: wrap;
      margin-top: 8px;
    }

    .links a {
      font-size: 14px;
      padding: 6px 10px;
      border-radius: 8px;
      background: color-mix(in oklab, var(--text) 5%, transparent);
      transition: all 0.2s ease;
    }

    .links a:hover {
      background: var(--accent);
      color: white;
      transform: translateY(-1px);
    }

    /* Footer */
    footer {
      margin: 60px 0 20px;
      color: var(--muted);
      font-size: 14px;
      text-align: center;
      padding: 20px 0;
      border-top: 1px solid color-mix(in oklab, var(--text) 5%, transparent);
    }

    /* Theme toggle */
    .toggle {
      cursor: pointer;
      border: 1px solid color-mix(in oklab, var(--text) 14%, transparent);
      padding: 8px 12px;
      border-radius: 12px;
      background: var(--card);
      transition: all 0.2s ease;
      font-size: 16px;
    }

    .toggle:hover {
      transform: rotate(15deg);
      border-color: var(--accent);
    }
  </style>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Yunjie Tian",
    "alternateName": "ç”°è¿æ°",
    "jobTitle": "Postdoctoral Associate",
    "affiliation": {"@type": "Organization", "name": "University at Buffalo"},
    "email": "mailto:tianyunjie96@gmail.com",
    "image": "files/susu.jpg",
    "url": "./",
    "sameAs": [
      "https://scholar.google.com/citations?user=DooPOjIAAAAJ&hl=en",
      "https://github.com/sunsmarterjie"
    ]
  }
  </script>
</head>

<body>
  <header>
    <div class="container nav">
      <div class="brand">
        <span class="dot" aria-hidden="true"></span>
        <a href="#top" style="font-weight:700">Yunjie Tian</a>
      </div>
      <nav style="display:flex; gap:8px; align-items:center">
        <a href="#about">About</a>
        <a href="#pubs">Publications</a>
        <a href="#contact">Contact</a>
        <button class="toggle" id="themeToggle" title="Toggle theme" aria-label="Toggle theme">ğŸŒ“</button>
      </nav>
    </div>
  </header>

  <main id="top" class="container">
    <!-- HERO -->
    <section class="hero grid cols-2">
      <div>
        <h1 class="title">Yunjie Tian <span style="opacity:.7">Â· ç”°è¿æ°</span></h1>
        <p class="subtitle">Postdoctoral Associate, <a href="https://www.buffalo.edu/" target="_blank" rel="noopener">University at Buffalo</a></p>
        <div class="chips">
          <span class="chip">Computer Vision</span>
          <span class="chip">Multimodal Learning</span>
          <span class="chip">Representation Learning</span>
          <span class="chip">Realâ€‘time Detection</span>
        </div>
        <div class="actions">
          <a class="btn" href="mailto:tianyunjie96@gmail.com">âœ‰ï¸ Email</a>
          <a class="btn" href="https://scholar.google.com/citations?user=DooPOjIAAAAJ&hl=en" target="_blank" rel="noopener">ğŸ“ Google Scholar</a>
          <a class="btn" href="https://github.com/sunsmarterjie" target="_blank" rel="noopener"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="width:24px;height:24px;vertical-align:middle;border-radius:5px;"> GitHub</a>
          <a class="btn" href="files/CV_Yunjie_Tian.pdf" target="_blank" rel="noopener">ğŸ“„ CV</a>
        </div>
        <p style="margin-top:14px;color:var(--muted)">I will be joining TikTok as a Senior Research Scientist. I am currently working with <a href="https://cse.buffalo.edu/~doermann/" target="_blank" rel="noopener">Prof. David Doermann</a> at UB. I received my Ph.D. from <a href="http://lamp.ucas.ac.cn/" target="_blank" rel="noopener">LAMP, UCAS</a> in 2024, advised by <a href="https://scholar.google.com.hk/citations?user=tjEfgsEAAAAJ" target="_blank" rel="noopener">Prof. Qixiang Ye</a>. B.E. from Jilin University (2019).</p>
      </div>
      <div>
        <img src="files/susu.jpg" alt="Portrait of Yunjie Tian" loading="eager" width="360" />
      </div>
    </section>

    <!-- ABOUT -->
    <section id="about" class="grid">
      <div class="card shadow">
        <h2>Research Focus</h2>
        <p>My research spans <strong>vision foundation models</strong>, <strong>multimodal LLMs</strong>, and <strong>efficient realâ€‘time perception</strong>. I design architectures and training strategies that make models <strong><em>faster</em></strong>, <strong><em>simpler</em></strong>, and <strong><em>clearer</em></strong> for visual and multimodal signals.</p>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="pubs">
      <h2>Selected Publications</h2>
      <div class="pubs">
        <!-- YOLOv12 -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/yolo12.png" alt="YOLOv12 thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Qixiang Ye, David Doermann</div>
            <div><b>YOLOv12: Attentionâ€‘Centric Realâ€‘Time Object Detectors</b></div>
            <div class="venue"><span class="pill">NeurIPS 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2502.12524" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/yolov12" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- AKS -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/aks.png" alt="AKS thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Xi Tang, Jihao Qiu, Lingxi Xie, <b>Yunjie Tian</b>, Jianbin Jiao, Qixiang Ye</div>
            <div><b>Adaptive Keyframe Sampling for Long Video Understanding</b></div>
            <div class="venue"><span class="pill">CVPR 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/pdf/2502.21271" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/ncTimTang/AKS" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- Artemis -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/artemis.png" alt="Artemis thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye, <b>Yunjie Tian</b></div>
            <div><b>Artemis: Towards Referential Understanding in Complex Videos</b></div>
            <div class="venue"><span class="pill">NeurIPS 2024</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2406.00258" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/qiujihao19/Artemis" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- vHeat -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/vheat.png" alt="vHeat thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Zhaozhi Wang, Yue Liu, Yunfan Liu, Hongtian Yu, Yaowei Wang, Qixiang Ye, <b>Yunjie Tian</b></div>
            <div><b>vHeat: Building Vision Models upon Heat Conduction</b></div>
            <div class="venue"><span class="pill">CVPR 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2405.16555" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/MzeroMiko/vHeat" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- ClawMachine -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/clawmachine.png" alt="ClawMachine thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Tianren Ma, Lingxi Xie, <b>Yunjie Tian</b>, Boyu Yang, Qixiang Ye</div>
            <div><b>ClawMachine: Fetching Visual Tokens as an Entity for Referring and Grounding</b></div>
            <div class="venue"><span class="pill">ICLR 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2406.11327" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/martian422/ClawMachine" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- ChatterBox -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/chatterbox.png" alt="ChatterBox thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Tianren Ma, Lingxi Xie, Qixiang Ye</div>
            <div><b>ChatterBox: Multiâ€‘round Multimodal Referring and Grounding</b></div>
            <div class="venue"><span class="pill">AAAI 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/pdf/2401.13307.pdf" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/ChatterBox" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- Beyond Masking -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/beyond_masking.png" alt="Beyond Masking thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Lingxi Xie, Mengnan Shi, Jiemin Fang, Xiaopeng Zhang, Jianbin Jiao, Qi Tian, Qixiang Ye</div>
            <div><b>Beyond Masking: Demystifying Tokenâ€‘Based Preâ€‘Training for Vision Transformers</b></div>
            <div class="venue"><span class="pill">Pattern Recognition 2025</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2203.14313" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/beyond_masking" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- VMamba -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/vmamba.png" alt="VMamba thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Yue Liu, <b>Yunjie Tian</b>, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Yunfan Liu</div>
            <div><b>VMamba: Visual State Space Model</b></div>
            <div class="venue">
              <span class="pill">NeurIPS 2024</span>
              <a class="pill" href="https://www.paperdigest.org/2025/09/most-influential-nips-papers-2025-09-version/" target="_blank" rel="noopener">
                Most Influential #2
              </a>
            </div>
            <div class="links">
              <a href="https://arxiv.org/abs/2401.10166" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/MzeroMiko/VMamba" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>


        <!-- Fast-iTPN -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/fast_itpn.png" alt="Fast-iTPN thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Lingxi Xie, Jihao Qiu, Jianbin Jiao, Qi Tian, Qixiang Ye</div>
            <div><b>Fastâ€‘iTPN: Integrally Preâ€‘Trained Transformer Pyramid Network with Token Migration</b></div>
            <div class="venue"><span class="pill">TPAMI 2024</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2211.12735" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/iTPN/tree/main/fast_itpn" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- IFNAS -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/ifnas.png" alt="IFNAS thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Lingxi Xie, Jiemin Fang, Jianbin Jiao, Qixiang Ye, Qi Tian</div>
            <div><b>Exploring Complicated Search Spaces with Interleavingâ€‘Free Sampling</b></div>
            <div class="venue"><span class="pill">TNNLS 2024</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2112.02488" target="_blank" rel="noopener">Paper</a>
            </div>
          </div>
        </article>

        <!-- STD -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/std.png" alt="STD thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">*Hongtian Yu, *<b>Yunjie Tian</b>, Qixiang Ye, Yunfan Liu</div>
            <div><b>Spatial Transform Decoupling for Oriented Object Detection</b></div>
            <div class="venue"><span class="pill">AAAI 2024</span></div>
            <div class="links">
              <a href="https://arxiv.org/abs/2308.10561" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/yuhongtian17/Spatial-Transform-Decoupling" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- iTPN -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/itpn.png" alt="iTPN thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Qi Tian, Qixiang Ye</div>
            <div><b>Integrally Preâ€‘Trained Transformer Pyramid Networks</b></div>
            <div class="venue"><span class="pill">CVPR 2023</span></div>
            <div class="links">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Integrally_Pre-Trained_Transformer_Pyramid_Networks_CVPR_2023_paper.pdf" target="_blank" rel="noopener">Paper</a>
              <a href="https://www.bilibili.com/video/BV18L411m7jU/?spm_id_from=333.337.search-card.all.click" target="_blank" rel="noopener">Presentation</a>
              <a href="https://github.com/sunsmarterjie/iTPN" target ="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- HiViT -->
        <article class="pub">
            <img class="thumb" src="files/PaperFig/hivit.png" alt="HiViT thumbnail" loading="lazy" />
            <div class="meta">
              <div class="authors">*Xiaosong Zhang, *<b>Yunjie Tian</b>, Wei Huang, Lingxi Xie, Qi Dai, Qixiang Ye, Qi Tian</div>
              <div><b>HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer</b></div>
              <div class="venue"><span class="pill">ICLR 2023</span></div>
              <div class="links">
                <a href="https://openreview.net/forum?id=3F6I-0-57SC" target="_blank" rel="noopener">Paper</a>
                <a href="https://github.com/zhangxiaosong18/hivit" target="_blank" rel="noopener">Code</a>
                <a href="https://arxiv.org/abs/2205.14949" target="_blank" rel="noopener">arXiv</a>
              </div>
            </div>
        </article>

        <!-- GraFormer -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/graformer.png" alt="GraFormer thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">Weixi Zhao, Weiqiang Wang, <b>Yunjie Tian</b></div>
            <div><b>GraFormer: Graphâ€‘Oriented Transformer for 3D Pose Estimation</b></div>
            <div class="venue"><span class="pill">CVPR 2022</span></div>
            <div class="links">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_GraFormer_Graph-Oriented_Transformer_for_3D_Pose_Estimation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/Graformer/GraFormer" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- DAAS -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/daas.png" alt="DAAS thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors"><b>Yunjie Tian</b>, Chang Liu, Lingxi Xie, Qixiang Ye</div>
            <div><b>Discretizationâ€‘Aware Architecture Search</b></div>
            <div class="venue"><span class="pill">Pattern Recognition 2021</span></div>
            <div class="links">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321003733" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/DAAS" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

        <!-- ADALSN -->
        <article class="pub">
          <img class="thumb" src="files/PaperFig/adalsn.png" alt="ADALSN thumbnail" loading="lazy" />
          <div class="meta">
            <div class="authors">*Chang Liu, *<b>Yunjie Tian</b>, Zhiwen Chen, Jianbin Jiao, Qixiang Ye</div>
            <div><b>Adaptive Linear Span Network for Object Skeleton Detection</b></div>
            <div class="venue"><span class="pill">TIP 2021</span></div>
            <div class="links">
              <a href="https://arxiv.org/pdf/2011.03972.pdf" target="_blank" rel="noopener">Paper</a>
              <a href="https://github.com/sunsmarterjie/SDL-Skeleton" target="_blank" rel="noopener">Code</a>
            </div>
          </div>
        </article>

      </div>
    </section>

    <!-- CONTACT / FOOTER -->
    <section id="contact" class="grid">
      <div class="card shadow">
        <h2>Contact</h2>
        <p>Room 301B, Davis Hall<br>Department of Computer Science and Engineering<br>University at Buffalo, SUNY<br>Buffalo, NY 14260, USA</p>
        <p>Email: <a href="mailto:tianyunjie96@gmail.com">tianyunjie96@gmail.com</a></p>
      </div>
    </section>

    <footer>
      <div class="container">
        <p>Â© <span id="year"></span> Yunjie Tian Â· ç”°è¿æ°</p>
        <p style="margin-top: 8px; font-size: 12px; opacity: 0.7;">Crafted with care in Buffalo.</p>
      </div>
    </footer>
  </main>

  <script>
    // Year
    document.getElementById('year').textContent = new Date().getFullYear();
    // Theme toggle (persist)
    const key = 'yt-theme';
    const toggle = document.getElementById('themeToggle');
    const apply = (t)=>{
      if(t==='light'){document.documentElement.style.colorScheme='light';}
      else if(t==='dark'){document.documentElement.style.colorScheme='dark';}
      else{document.documentElement.style.colorScheme='normal';}
    }
    const saved = localStorage.getItem(key);
    if(saved) apply(saved);
    toggle.addEventListener('click', ()=>{
      const cur = localStorage.getItem(key) || (matchMedia('(prefers-color-scheme: dark)').matches? 'dark':'light');
      const next = cur==='dark' ? 'light' : 'dark';
      localStorage.setItem(key, next); apply(next);
    });
  </script>
</body>
</html>
